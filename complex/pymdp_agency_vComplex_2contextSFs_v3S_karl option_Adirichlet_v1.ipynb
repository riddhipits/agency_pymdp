{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active inference model of agency task\n",
    "## complex version with changed context that's being inferred by agent\n",
    "\n",
    "#### prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install inferactively-pymdp\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pymdp\n",
    "\n",
    "from pymdp import utils \n",
    "from pymdp import maths\n",
    "from pymdp.maths import softmax\n",
    "from pymdp.agent import Agent\n",
    "from pymdp.utils import plot_beliefs, plot_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE GENERATIVE MODEL\n",
    "\n",
    "#### specifying the states and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining state factors \"\"\"\n",
    "self_context_names = ['self_positivecontrol', 'self_negativecontrol', 'self_zerocontrol']\n",
    "other_context_names = ['other_positivecontrol', 'other_negativecontrol', 'other_zerocontrol']\n",
    "self_action_names = ['self_buttonpress', 'self_buttonnotpress']\n",
    "other_action_names = ['other_buttonpress', 'other_buttonnotpress']\n",
    "\n",
    "\"\"\" Defining number of state factors and states \"\"\"\n",
    "num_states = [len(self_context_names), len(other_context_names), len(self_action_names), len(other_action_names)]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "\"\"\" Defining control state factors \"\"\"\n",
    "choice_self_context_names = ['no_changes']\n",
    "choice_other_context_names = ['no_changes']\n",
    "choice_self_action_names = ['self_pressbutton', 'self_notpressbutton']\n",
    "choice_other_action_names = ['equal_distribution']\n",
    "\n",
    "\"\"\" Defining number of control states \"\"\"\n",
    "num_controls = [len(choice_self_context_names), len(choice_other_context_names), \n",
    "                len(choice_self_action_names), len(choice_other_action_names)]\n",
    "\n",
    "\"\"\" Defining observational modalities \"\"\"\n",
    "obs_outcome_names = ['outcome_present', 'outcome_absent']\n",
    "obs_choice_self_names = ['self_buttonpress', 'self_buttonnotpress']\n",
    "obs_choice_other_names = ['other_buttonpress', 'other_buttonnotpress']\n",
    "\n",
    "\"\"\" Defining number of observational modalities and observations \"\"\"\n",
    "num_obs = [len(obs_outcome_names), len(obs_choice_self_names), len(obs_choice_other_names)]\n",
    "num_modalities = len(num_obs)\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### likelihood (A) tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3, 2, 2)\n",
      "[[[[[1.  0. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[0.  1. ]\n",
      "    [0.  0. ]]\n",
      "\n",
      "   [[1.  1. ]\n",
      "    [0.  0. ]]]\n",
      "\n",
      "\n",
      "  [[[0.  0. ]\n",
      "    [1.  0. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [0.  1. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [1.  1. ]]]\n",
      "\n",
      "\n",
      "  [[[1.  0. ]\n",
      "    [1.  0. ]]\n",
      "\n",
      "   [[0.  1. ]\n",
      "    [0.  1. ]]\n",
      "\n",
      "   [[0.5 0.5]\n",
      "    [0.5 0.5]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[0.  1. ]\n",
      "    [1.  1. ]]\n",
      "\n",
      "   [[1.  0. ]\n",
      "    [1.  1. ]]\n",
      "\n",
      "   [[0.  0. ]\n",
      "    [1.  1. ]]]\n",
      "\n",
      "\n",
      "  [[[1.  1. ]\n",
      "    [0.  1. ]]\n",
      "\n",
      "   [[1.  1. ]\n",
      "    [1.  0. ]]\n",
      "\n",
      "   [[1.  1. ]\n",
      "    [0.  0. ]]]\n",
      "\n",
      "\n",
      "  [[[0.  1. ]\n",
      "    [0.  1. ]]\n",
      "\n",
      "   [[1.  0. ]\n",
      "    [1.  0. ]]\n",
      "\n",
      "   [[0.5 0.5]\n",
      "    [0.5 0.5]]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riddhi_rjp/opt/anaconda3/lib/python3.8/site-packages/pymdp/utils.py:166: UserWarning: Input array is not an object array...                    Casting the input to an object array\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def create_A(p_outcome):\n",
    "    \n",
    "    A = utils.obj_array(len(num_obs))\n",
    "    \n",
    "#     ''' A matrix for outcome '''\n",
    "    \n",
    "#     A_outcome = np.empty( (1, len(obs_outcome_names), len(self_context_names),\n",
    "#                            len(other_context_names), len(self_action_names),\n",
    "#                            len(other_action_names)) )\n",
    "    \n",
    "#     A_outcome = A_outcome.astype(object)\n",
    "    \n",
    "#     A_outcome[0,:,:,:,:] = 0.5\n",
    "#     A_outcome[:,0,:,:,:] = 0.5\n",
    "#     A_outcome[:,:,0,:,:] = 0.5\n",
    "#     A_outcome[:,:,:,0,:] = 0.5\n",
    "#     A_outcome[:,:,:,:,0] = 0.5\n",
    "\n",
    "    ''' A matrix for outcome '''\n",
    "    A_outcome = np.zeros( (len(obs_outcome_names), len(self_context_names), \n",
    "                           len(other_context_names), len(self_action_names), \n",
    "                           len(other_action_names)) )\n",
    "    \n",
    "    # SELF_POS, OTHER_POS\n",
    "    \n",
    "    # p(outcome | self_pos, other_pos, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,0,0,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_pos, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,0,1,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_pos, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,0,0,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_pos, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,0,1,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # SELF_POS, OTHER_NEG\n",
    "    \n",
    "    # p(outcome | self_pos, other_neg, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,1,0,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_neg, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,1,1,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_neg, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,1,0,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_neg, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,1,1,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # SELF_POS, OTHER_ZERO\n",
    "    \n",
    "    # p(outcome | self_pos, other_zero, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,2,0,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_zero, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,2,1,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_zero, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,2,0,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_pos, other_zero, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,0,2,1,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # SELF_NEG, OTHER_POS\n",
    "    \n",
    "    # p(outcome | self_neg, other_pos, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,0,0,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_pos, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,0,1,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_pos, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,0,0,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_pos, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,0,1,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # SELF_NEG, OTHER_NEG\n",
    "    \n",
    "    # p(outcome | self_neg, other_neg, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,1,0,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_neg, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,1,1,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_neg, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,1,0,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_neg, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,1,1,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # SELF_NEG, OTHER_ZERO\n",
    "    \n",
    "    # p(outcome | self_neg, other_zero, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,2,0,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_zero, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,2,1,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_zero, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,2,0,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_neg, other_zero, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,1,2,1,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    \n",
    "    # SELF_ZERO, OTHER_POS\n",
    "    \n",
    "    # p(outcome | self_zero, other_pos, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,0,0,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_pos, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,0,1,0] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_pos, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,0,0,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_pos, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,0,1,1] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # SELF_ZERO, OTHER_NEG\n",
    "    \n",
    "    # p(outcome | self_zero, other_neg, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,1,0,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_neg, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,1,1,0] = [1.0-p_outcome, p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_neg, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,1,0,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # p(outcome | self_zero, other_neg, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,1,1,1] = [p_outcome, 1.0-p_outcome]\n",
    "    \n",
    "    # SELF_ZERO, OTHER_ZERO\n",
    "    \n",
    "    # p(outcome | self_zero, other_zero, self_press, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,2,0,0] = [0.5, 0.5]\n",
    "    \n",
    "    # p(outcome | self_zero, other_zero, self_notpress, other_press)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,2,1,0] = [0.5, 0.5]\n",
    "    \n",
    "    # p(outcome | self_zero, other_zero, self_press, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,2,0,1] = [0.5, 0.5]\n",
    "    \n",
    "    # p(outcome | self_zero, other_zero, self_notpress, other_notpress)\n",
    "    #                       present    absent\n",
    "    A_outcome[:,2,2,1,1] = [0.5, 0.5]\n",
    "    \n",
    "    pA = utils.dirichlet_like(A_outcome, scale = 1e16)\n",
    "    \n",
    "    ''' A matrix for proprioception '''\n",
    "    A_choice_self = np.zeros((len(obs_choice_self_names), len(self_action_names)))\n",
    "    \n",
    "    A_choice_self = np.eye(len(self_action_names))\n",
    "    \n",
    "    ''' A matrix for observing other agent's actions '''\n",
    "    A_choice_other = np.zeros((len(obs_choice_other_names), len(other_action_names)))\n",
    "    \n",
    "    A_choice_other = np.eye(len(other_action_names))\n",
    "    \n",
    "    ''' stacking up the A matrices '''\n",
    "    A[0], A[1], A[2] = A_outcome, A_choice_self, A_choice_other\n",
    "    A_factor_list = [[0,1,2,3], [2], [3]]\n",
    "    \n",
    "    A = utils.norm_dist_obj_arr(A)\n",
    "    \n",
    "    return A, A_factor_list, pA\n",
    "\n",
    "# printing matrices out for testing\n",
    "A,A_factor_list,pA = create_A(p_outcome = 1.0)\n",
    "\n",
    "print(A[0].shape)\n",
    "print(A[0])\n",
    "# plot_likelihood(A[0][:,:,0,0,1],'Initial beliefs')\n",
    "\n",
    "learnable_modalities = [0] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### state transition (B) tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_B():\n",
    "    \n",
    "    B = utils.initialize_empty_B(num_states, num_states)\n",
    "    \n",
    "    ''' B matrix for context regarding the self '''\n",
    "    B_self_context = np.zeros( (len(self_context_names), len(self_context_names), len(choice_self_context_names)) )\n",
    "\n",
    "#     B_self_context[:,:,0] = np.array(1.0/float(num_states[0]))\n",
    "    B_self_context[:,:,0] = np.eye(len(self_context_names))\n",
    "    \n",
    "#     B_self_context = softmax(B_self_context)\n",
    "    \n",
    "    ''' B matrix for context regarding the other '''\n",
    "    B_other_context = np.zeros( (len(other_context_names), len(other_context_names), len(choice_other_context_names)) )\n",
    "\n",
    "#     B_other_context[:,:,0] = np.array(1.0/float(num_states[1]))\n",
    "    B_other_context[:,:,0] = np.eye(len(other_context_names))\n",
    "    \n",
    "#     B_other_context = softmax(B_other_context)\n",
    "    \n",
    "    ''' B matrix for actions the self can make '''\n",
    "    B_self_choice = np.zeros( (len(self_action_names), len(self_action_names), len(choice_self_action_names)) )\n",
    "    \n",
    "    for choice_id in range(len(self_action_names)):\n",
    "        B_self_choice[choice_id, :, choice_id] = 1.0\n",
    "    \n",
    "    ''' B matrix for actions the other can make '''\n",
    "    B_other_choice = np.zeros( (len(other_action_names), len(other_action_names), len(choice_other_action_names)) )\n",
    "    \n",
    "    B_other_choice[:,:,0] = np.array(1.0/float(num_states[3]))\n",
    "    \n",
    "    ''' stacking up the B matrices '''\n",
    "    B[0], B[1], B[2], B[3] = B_self_context, B_other_context, B_self_choice, B_other_choice\n",
    "    \n",
    "    B = utils.norm_dist_obj_arr(B)\n",
    "    \n",
    "    return B\n",
    "\n",
    "# B = create_B()\n",
    "# print(B[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preferences (C) vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_C(outcomepref, actionpref, noactionpref):\n",
    "        \n",
    "    C = utils.obj_array_zeros(num_obs)\n",
    "    C[0] = np.array([outcomepref, 0.0])\n",
    "    C[1] = np.array([actionpref, noactionpref])\n",
    "    C[2] = np.array([0.0, 0.0])\n",
    "    \n",
    "    return C\n",
    "\n",
    "# C = create_C(reward = 1.0, pun = 0.0)\n",
    "# utils.plot_beliefs(softmax(C[0]), title = \"Prior preferences for outcome_present and outcome_absent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### priors (D) vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_D():\n",
    "    \n",
    "    D = utils.obj_array(num_factors)\n",
    "    \n",
    "    D_self_context = np.ones(num_states[0])/float(num_states[0])\n",
    "    \n",
    "    D_other_context = np.ones(num_states[1])/float(num_states[1])\n",
    "    \n",
    "    D_self_choice = np.zeros(len(self_action_names)) \n",
    "    D_self_choice = np.ones(num_states[2])/float(num_states[2])\n",
    "    \n",
    "    D_other_choice = np.zeros(len(other_action_names)) \n",
    "    D_other_choice = np.ones(num_states[3])/float(num_states[3])\n",
    "\n",
    "    D[0], D[1], D[2], D[3] = D_self_context, D_other_context, D_self_choice, D_other_choice\n",
    "    \n",
    "    return D\n",
    "\n",
    "# D = create_D(p_context=1/float(num_states[0]))\n",
    "# utils.plot_beliefs(softmax(D[1]), title = \"Prior beliefs about probability of the contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE GENERATIVE PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgencyTask(object):\n",
    "    \n",
    "    def __init__(self, expcondition = None , p_other_action = 0.5, p_outcome = 1.0):\n",
    "        \n",
    "        # defining the experimental conditions for the generative process\n",
    "        self.expcondition_names = ['s_pos_o_zer', 's_neg_o_zer',\n",
    "                                   's_zer_o_pos', 's_zer_o_neg', \n",
    "                                   's_zer_o_zer']\n",
    "        \n",
    "        self.expcondition_full_names = ['s_pos_o_pos', 's_pos_o_neg', 's_pos_o_zer',\n",
    "                                        's_neg_o_pos', 's_neg_o_neg', 's_neg_o_zer', \n",
    "                                        's_zer_o_pos', 's_zer_o_neg', 's_zer_o_zer']\n",
    "        \n",
    "        self.num_expcondition = len(self.expcondition_names)\n",
    "        \n",
    "        if expcondition == None:\n",
    "            self.expcondition = self.expcondition_names[utils.sample(np.ones(self.num_expcondition)/self.num_expcondition)] # randomly sample which context is selected\n",
    "        else:\n",
    "            self.expcondition = expcondition\n",
    "            \n",
    "        self.p_outcome = p_outcome\n",
    "        self.p_other_action = p_other_action\n",
    "        \n",
    "        self.action_other_names = ['other_pressbutton', 'other_notpressbutton']\n",
    "        self.obs_outcome_names = ['outcome_present', 'outcome_absent']\n",
    "                \n",
    "    def step(self, action):\n",
    "\n",
    "        # sampling the other agent's actions at random (p(other_action) = 0.5)\n",
    "        self.action_other = self.action_other_names[utils.sample(np.array([self.p_other_action, 1.0-self.p_other_action]))]\n",
    "                \n",
    "        if action == \"self_pressbutton\" and self.action_other == \"other_pressbutton\":\n",
    "            observed_choice_self = \"self_buttonpress\"\n",
    "            observed_choice_other = \"other_buttonpress\"\n",
    "            \n",
    "            if self.expcondition == 's_pos_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.5, 0.5]))]\n",
    "            elif self.expcondition == 's_pos_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_pos_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "                \n",
    "        elif action == \"self_pressbutton\" and self.action_other == \"other_notpressbutton\":\n",
    "            observed_choice_self = \"self_buttonpress\"\n",
    "            observed_choice_other = \"other_buttonnotpress\"\n",
    "            \n",
    "            if self.expcondition == 's_pos_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.5, 0.5]))]\n",
    "            elif self.expcondition == 's_pos_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_pos_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "                \n",
    "        elif action == \"self_notpressbutton\" and self.action_other == \"other_pressbutton\":\n",
    "            observed_choice_self = \"self_buttonnotpress\"\n",
    "            observed_choice_other = \"other_buttonpress\"\n",
    "            \n",
    "            if self.expcondition == 's_pos_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.5, 0.5]))]\n",
    "            elif self.expcondition == 's_pos_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_pos_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "                \n",
    "        elif action == \"self_notpressbutton\" and self.action_other == \"other_notpressbutton\":\n",
    "            observed_choice_self = \"self_buttonnotpress\"\n",
    "            observed_choice_other = \"other_buttonnotpress\"\n",
    "            \n",
    "            if self.expcondition == 's_pos_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.expcondition == 's_zer_o_zer':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.5, 0.5]))]\n",
    "            elif self.expcondition == 's_pos_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_pos_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_pos':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.expcondition == 's_neg_o_neg':\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([self.p_outcome, 1.0-self.p_outcome]))]\n",
    "\n",
    "        obs = [observed_outcome, observed_choice_self, observed_choice_other]\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING ACTIVE INFERENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_inference_loop(my_agent, my_env, T, verbose):\n",
    "\n",
    "    choice_self_hist = np.zeros((num_controls[2],T))\n",
    "    \n",
    "    belief_self_context_hist = np.zeros((num_states[0], T))\n",
    "    belief_other_context_hist = np.zeros((num_states[1], T))\n",
    "    belief_self_action_hist = np.zeros((num_states[2], T))\n",
    "    belief_other_action_hist = np.zeros((num_states[3], T))\n",
    "    \n",
    "    expcondition_hist = np.zeros(T)\n",
    "    outcome_hist = np.zeros((num_obs[0],T))\n",
    "    \n",
    "    pA_history = []\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        q_pi, efe = my_agent.infer_policies_factorized()\n",
    "        \n",
    "        chosen_action_id = my_agent.sample_action()\n",
    "        action_id = int(chosen_action_id[2])\n",
    "        choice_self_hist[action_id,t]= 1.0\n",
    "        choice_action = choice_self_action_names[action_id]\n",
    "        \n",
    "        pA_t = agent.update_A(obs)\n",
    "        pA_history.append(pA_t)\n",
    "          \n",
    "        obs_label = my_env.step(choice_action)\n",
    "        obs = [obs_outcome_names.index(obs_label[0]), obs_choice_self_names.index(obs_label[1]), obs_choice_other_names.index(obs_label[2])]\n",
    "        \n",
    "        expcondition_hist[t] = env.expcondition_names.index(env.expcondition)\n",
    "        \n",
    "        qs = my_agent.infer_states(obs)\n",
    "        belief_self_context_hist[:,t] = qs[0]\n",
    "        belief_other_context_hist[:,t] = qs[1]\n",
    "        belief_self_action_hist[:,t] = qs[2]\n",
    "        belief_other_action_hist[:,t] = qs[3]\n",
    "        \n",
    "        outcome_hist[obs[0],t] = 1.0\n",
    "            \n",
    "    return choice_self_hist, belief_self_context_hist, belief_other_context_hist, expcondition_hist, belief_other_action_hist, outcome_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b312a071214c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutcomepref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactionpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoactionpref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m my_agent = Agent(A=A, pA=pA, B=B, C=C, D=D, A_factor_list=A_factor_list, \n\u001b[0m\u001b[1;32m     13\u001b[0m                  \u001b[0mmodalities_to_learn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearnable_modalities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_pA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                  use_param_info_gain=True)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pymdp/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, A, B, C, D, E, pA, pB, pD, num_controls, policy_len, inference_horizon, control_fac_idx, policies, gamma, alpha, use_utility, use_states_info_gain, use_param_info_gain, action_selection, sampling_mode, inference_algo, inference_params, modalities_to_learn, lr_pA, factors_to_learn, lr_pB, lr_pD, use_BMA, policy_sep_prior, save_belief_hist, A_factor_list, B_factor_list)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfactor_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Check modality {m} of A_factor_list. It must coincide with lagging dimensions of A{m}...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpA\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                     \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfactor_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Check modality {m} of A_factor_list. It must coincide with lagging dimensions of pA{m}...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA_factor_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_factor_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "p_outcome_env = 1.0\n",
    "p_other_action_env = 0.5\n",
    "\n",
    "env = AgencyTask(p_other_action = p_other_action_env, p_outcome = p_outcome_env)\n",
    "\n",
    "T = 20\n",
    "\n",
    "A,A_factor_list,pA = create_A(p_outcome = 1.0)\n",
    "B = create_B()\n",
    "C = create_C(outcomepref = 5.0, actionpref = 0.0, noactionpref = 0.05)\n",
    "D = create_D()\n",
    "my_agent = Agent(A=A, pA=pA, B=B, C=C, D=D, A_factor_list=A_factor_list, \n",
    "                 modalities_to_learn=learnable_modalities, lr_pA = 0.25,\n",
    "                 use_param_info_gain=True)\n",
    "\n",
    "\n",
    "choice_self_hist, belief_self_context_hist, belief_other_context_hist, expcondition_hist, belief_other_action_hist, outcome_hist = run_active_inference_loop(my_agent, env, T = T, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_choices_beliefs(choice_self_hist, belief_self_context_hist, belief_other_context_hist, pad_val=5.0):\n",
    "    print(env.expcondition)\n",
    "    \n",
    "    T = choice_self_hist.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 3, ncols = 1, figsize = (15,13))\n",
    "    \n",
    "    axes[0].imshow(choice_self_hist[:,:-1], cmap = 'gray') \n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_yticks(ticks = range(num_states[2]))\n",
    "    axes[0].set_yticklabels(labels = choice_action_self_names)\n",
    "    axes[0].set_title('Actions produced by the self over time')\n",
    "    \n",
    "    axes[1].imshow(belief_self_context_hist, cmap = 'gray')\n",
    "    axes[1].set_xlabel('Timesteps')\n",
    "    axes[1].set_yticks(ticks = range(num_states[0]))\n",
    "    axes[1].set_yticklabels(labels = context_self_names)\n",
    "    axes[1].set_title('Beliefs about control the self has over time')\n",
    "    \n",
    "    axes[2].imshow(belief_other_context_hist, cmap = 'gray')\n",
    "    axes[2].set_xlabel('Timesteps')\n",
    "    axes[2].set_yticks(ticks = range(num_states[1]))\n",
    "    axes[2].set_yticklabels(labels = context_other_names)\n",
    "    axes[2].set_title('Beliefs about control the other has over time')\n",
    "    \n",
    "    fig.tight_layout(pad=pad_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_choices_beliefs(choice_self_hist, belief_self_context_hist, belief_other_context_hist, expcondition_hist, belief_other_action_hist, outcome_hist, pad_val=1.0):\n",
    "    \n",
    "    print(f'Experimental Condition (or Context): {env.expcondition}')\n",
    "    \n",
    "    T = choice_self_hist.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 5, ncols = 1, figsize = (15,20))\n",
    "    \n",
    "    axes[0].imshow(belief_self_context_hist, cmap = 'gray')\n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_yticks(ticks = range(num_states[0]))\n",
    "    axes[0].set_yticklabels(labels = self_context_names)\n",
    "    axes[0].set_title('Beliefs about control the self has over time')\n",
    "\n",
    "    axes[1].imshow(belief_other_context_hist, cmap = 'gray')\n",
    "    axes[1].set_xlabel('Timesteps')\n",
    "    axes[1].set_yticks(ticks = range(num_states[1]))\n",
    "    axes[1].set_yticklabels(labels = other_context_names)\n",
    "    axes[1].set_title('Beliefs about control the other has over time')\n",
    "\n",
    "    axes[2].imshow(choice_self_hist[:,:-1], cmap = 'gray') \n",
    "    axes[2].set_xlabel('Timesteps')\n",
    "    axes[2].set_yticks(ticks = range(num_states[2]))\n",
    "    axes[2].set_yticklabels(labels = self_action_names)\n",
    "    axes[2].set_title('Actions produced by the self over time')\n",
    "    \n",
    "    axes[3].imshow(belief_other_action_hist[:,:-1], cmap = 'gray') \n",
    "    axes[3].set_xlabel('Timesteps')\n",
    "    axes[3].set_yticks(ticks = range(num_states[3]))\n",
    "    axes[3].set_yticklabels(labels = other_action_names)\n",
    "    axes[3].set_title('Beliefs about actions produced by the other over time')\n",
    "\n",
    "    axes[4].imshow(outcome_hist[:,:-1], cmap = 'gray') \n",
    "    axes[4].set_xlabel('Timesteps')\n",
    "    axes[4].set_yticks(ticks = range(num_obs[0]))\n",
    "    axes[4].set_yticklabels(labels = obs_outcome_names)\n",
    "    axes[4].set_title('Outcomes observed over time')\n",
    "\n",
    "    fig.tight_layout(pad=pad_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_some_choices_beliefs(choice_hist, belief_hist, context_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_choices_beliefs(choice_self_hist, belief_self_context_hist, belief_other_context_hist, expcondition_hist, belief_other_action_hist, outcome_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' TO PRINT INFO GAIN BASED ON POSTERIOR OF STATES IN A POLICY '''\n",
    "# from pymdp.control import (\n",
    "#         get_expected_states,\n",
    "#         get_expected_obs_factorized,\n",
    "#         calc_expected_utility,\n",
    "#         calc_states_info_gain_factorized,\n",
    "# )\n",
    "\n",
    "# def eval_policy(my_agent):\n",
    "    \n",
    "#     for idx, policy in enumerate(my_agent.policies):\n",
    "    \n",
    "#         qs_pi = get_expected_states(\n",
    "#             my_agent.qs, my_agent.B, policy\n",
    "#         )\n",
    "\n",
    "#         qo_pi = get_expected_obs_factorized(\n",
    "#             qs_pi, my_agent.A, my_agent.A_factor_list\n",
    "#         )\n",
    "\n",
    "#         e_u = calc_expected_utility(qo_pi, my_agent.C)\n",
    "\n",
    "#         e_ig = calc_states_info_gain_factorized(\n",
    "#             my_agent.A, qs_pi, my_agent.A_factor_list\n",
    "#         )\n",
    "\n",
    "#         EFE = e_u + e_ig\n",
    "\n",
    "#         print(e_ig)\n",
    "\n",
    "\n",
    "\n",
    "# ''' in the run_ai_loop: '''      \n",
    "#         eval_policy(my_agent)\n",
    "##         infogain = my_agent.calc_states_info_gain_factorized()\n",
    "#         print(t)\n",
    "#         print(q_pi.round(3))\n",
    "#         print(efe.round(3))\n",
    "#         print(infogain.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
