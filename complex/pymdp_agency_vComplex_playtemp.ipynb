{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active inference model of agency task (basic)\n",
    "\n",
    "### prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install inferactively-pymdp\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pymdp\n",
    "\n",
    "from pymdp import utils \n",
    "from pymdp import maths\n",
    "from pymdp.maths import softmax\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specifying the states and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Defining state factors \"\"\"\n",
    "context_self_names = ['positive_self', 'negative_self']\n",
    "context_other_names = ['positive_other', 'negative_other']\n",
    "action_self_names = ['start_self', 'Press_self', 'noPress_self']\n",
    "action_other_names = ['start_other', 'Press_other', 'noPress_other']\n",
    "\n",
    "\"\"\" Defining number of state factors and states \"\"\"\n",
    "num_states = [len(context_self_names), len(context_other_names), len(action_self_names), len(action_other_names)]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "\"\"\" Defining control state factors \"\"\"\n",
    "choice_context_self_names = ['no_changes']\n",
    "choice_context_other_names = ['no_changes']\n",
    "choice_action_self_names = ['start_self', 'Action_self', 'noAction_self']\n",
    "choice_action_other_names = ['equal_distribution']\n",
    "\n",
    "\"\"\" Defining number of control states \"\"\"\n",
    "num_controls = [len(choice_context_self_names), len(choice_context_other_names), len(choice_action_self_names), len(choice_action_other_names)]\n",
    "\n",
    "\"\"\" Defining observational modalities \"\"\"\n",
    "obs_outcome_names = ['outcome_start','outcome_present', 'outcome_absent']\n",
    "obs_choice_self_names = ['start_self', 'Press_self', 'noPress_self']\n",
    "obs_choice_other_names = ['start_other', 'Press_other', 'noPress_other']\n",
    "\n",
    "\"\"\" Defining number of observational modalities and observations \"\"\"\n",
    "num_obs = [len(obs_outcome_names), len(obs_choice_self_names), len(obs_choice_other_names)]\n",
    "num_modalities = len(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_A(p_outcome):\n",
    "    \n",
    "    A = utils.obj_array(len(num_obs))\n",
    "    \n",
    "    ''' A matrix for outcome '''\n",
    "    A_outcome = np.zeros( (len(obs_outcome_names), len(context_self_names), len(context_other_names), len(action_self_names), len(action_other_names)) )\n",
    "    \n",
    "    # p(outcome_start | action_self_start)\n",
    "    A_outcome[0,:,:,0,:] = 1.0\n",
    "\n",
    "    # p(outcome_start | action_other_start)\n",
    "    A_outcome[0,:,:,:,0] = 1.0\n",
    "    \n",
    "    # p(outcome_present | positive_self, Press_self)\n",
    "    A_outcome[1,0,:,1,:] = p_outcome\n",
    "    \n",
    "    # p(outcome_absent | positive_self, noPress_self)\n",
    "    A_outcome[2,0,:,2,:] = p_outcome\n",
    "    \n",
    "    # p(outcome_present | negative_self, noPress_self)\n",
    "    A_outcome[1,1,:,2,:] = p_outcome\n",
    "    \n",
    "    # p(outcome_absent | negative_self, Press_self)\n",
    "    A_outcome[2,1,:,1,:] = p_outcome\n",
    "    \n",
    "    # p(outcome_present | positive_other, Press_other)\n",
    "    A_outcome[1,:,0,:,1] = p_outcome\n",
    "    \n",
    "    # p(outcome_absent | positive_other, noPress_other)\n",
    "    A_outcome[2,:,0,:,2] = p_outcome\n",
    "    \n",
    "    # p(outcome_present | negative_other, noPress_other)\n",
    "    A_outcome[1,:,1,:,2] = p_outcome\n",
    "    \n",
    "    # p(outcome_absent | negative_other, Press_other)\n",
    "    A_outcome[2,:,1,:,1] = p_outcome\n",
    "    \n",
    "    ''' A matrix for proprioception '''\n",
    "    A_choice_self = np.zeros((len(obs_choice_self_names), len(action_self_names)))\n",
    "    \n",
    "    A_choice_self = np.eye(len(action_self_names))\n",
    "    \n",
    "    ''' A matrix for observing other agent's actions '''\n",
    "    A_choice_other = np.zeros((len(obs_choice_other_names), len(action_other_names)))\n",
    "    \n",
    "    A_choice_other = np.eye(len(action_other_names))\n",
    "    \n",
    "    ''' stacking up the A matrices '''\n",
    "    A[0], A[1], A[2] = A_outcome, A_choice_self, A_choice_other\n",
    "    A_factor_list = [[0,1,2,3], [2], [3]]\n",
    "    \n",
    "    A = utils.norm_dist_obj_arr(A)\n",
    "    \n",
    "    return A, A_factor_list\n",
    "\n",
    "\n",
    "# A = create_A(p_outcome = 1.0)\n",
    "# utils.plot_likelihood(A[0][:,:,0,0], title = \"P(outcome_start|start_self, start_other)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_B():\n",
    "    \n",
    "    B = utils.initialize_empty_B(num_states, num_states)\n",
    "    \n",
    "    B_context_self = np.zeros( (len(context_self_names), len(context_self_names), len(choice_context_self_names)) )\n",
    "#     B_context_self[:,:,0] = np.array(1.0/float(num_states[0]))\n",
    "    B_context_self[:,:,0] = np.eye(len(context_self_names))\n",
    "    \n",
    "    B_context_other = np.zeros( (len(context_other_names), len(context_other_names), len(choice_context_other_names)) )\n",
    "#     B_context_other[:,:,0] = np.array(1.0/float(num_states[1]))\n",
    "    B_context_other[:,:,0] = np.eye(len(context_other_names))\n",
    "    \n",
    "    B_choice_self = np.zeros( (len(action_self_names), len(action_self_names), len(choice_action_self_names)) )\n",
    "    \n",
    "    for choice_i in range(len(action_self_names)):\n",
    "        B_choice_self[choice_i, :, choice_i] = 1.0\n",
    "    \n",
    "    B_choice_other = np.zeros( (len(action_other_names), len(action_other_names), len(choice_action_other_names)) )\n",
    "    \n",
    "    for choice_i in range(len(action_other_names)):\n",
    "        B_choice_other[:,:,0] = np.array(1.0/float(num_states[3]))\n",
    "        \n",
    "    B[0], B[1], B[2], B[3] = B_context_self, B_context_other, B_choice_self, B_choice_other\n",
    "    \n",
    "    return B\n",
    "\n",
    "# B = create_B(p_change=0.0)\n",
    "# print(B[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_C(outcomepref, actionpref, noactionpref):\n",
    "    \n",
    "    # define reward and punishment values mapping onto outcome_present and outcome_absent preferences\n",
    "    \n",
    "    C = utils.obj_array_zeros(num_obs)\n",
    "    C[0] = np.array([0.0, outcomepref, 0.0])\n",
    "    C[1] = np.array([0.0, actionpref, noactionpref])\n",
    "    C[2] = np.array([0.0, 0.0, 0.0])\n",
    "    \n",
    "    return C\n",
    "\n",
    "# C = create_C(reward = 1.0, pun = 0.0)\n",
    "# utils.plot_beliefs(softmax(C[0]), title = \"Prior preferences for outcome_present and outcome_absent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_D():\n",
    "    \n",
    "    D = utils.obj_array(num_factors)\n",
    "    \n",
    "    D_context_self = np.ones(num_states[0])/float(num_states[0])\n",
    "    \n",
    "    D_context_other = np.ones(num_states[1])/float(num_states[1])\n",
    "    \n",
    "    D_choice_self = np.zeros(len(action_self_names)) \n",
    "    D_choice_self[action_self_names.index(\"start_self\")] = 1.0\n",
    "    \n",
    "    D_choice_other = np.zeros(len(action_other_names)) \n",
    "    D_choice_other[action_other_names.index(\"start_other\")] = 1.0\n",
    "\n",
    "    D[0], D[1], D[2], D[3] = D_context_self, D_context_other, D_choice_self, D_choice_other\n",
    "    \n",
    "    return D\n",
    "\n",
    "# D = create_D(p_context=1/float(num_states[0]))\n",
    "# utils.plot_beliefs(softmax(D[1]), title = \"Prior beliefs about probability of the contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgencyTask(object):\n",
    "    \n",
    "    def __init__(self, context = None, p_action_other = 0.5, p_outcome = 1.0):\n",
    "        \n",
    "        self.context_names = ['self_positive', 'self_negative',\n",
    "                              'other_positive', 'other_negative', 'zero']\n",
    "        if context == None:\n",
    "            self.context = self.context_names[utils.sample(np.ones(num_states[0])/float(num_states[0]))] # randomly sample which context is selected\n",
    "        else:\n",
    "            self.context = context\n",
    "            \n",
    "        self.p_outcome = p_outcome\n",
    "        self.p_action_other = p_action_other\n",
    "        \n",
    "        self.obs_outcome_names = ['outcome_start', 'outcome_present', 'outcome_absent']\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        self.action_other_names = ['start_other', 'Action_other', 'noAction_other']\n",
    "        self.action_other = self.action_other_names[utils.sample(np.array([0.0, self.p_action_other, 1.0-self.p_action_other]))]\n",
    "        \n",
    "        if action == \"start_self\" and self.action_other == \"start_other\":\n",
    "            observed_choice_self = \"start_self\"\n",
    "            observed_choice_other = \"start_other\"\n",
    "            observed_outcome = \"outcome_start\"\n",
    "                \n",
    "        elif action == \"Action_self\" and self.action_other == \"Action_other\":\n",
    "            observed_choice_self = \"Press_self\"\n",
    "            observed_choice_other = \"Press_other\"\n",
    "            \n",
    "            if self.context == \"self_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"self_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"other_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"other_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"Action_self\" and self.action_other == \"noAction_other\":\n",
    "            observed_choice_self = \"Press_self\"\n",
    "            observed_choice_other = \"noPress_other\"\n",
    "            \n",
    "            if self.context == \"self_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"self_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"other_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"other_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"noAction_self\" and self.action_other == \"Action_other\":\n",
    "            observed_choice_self = \"noPress_self\"\n",
    "            observed_choice_other = \"Press_other\"\n",
    "            \n",
    "            if self.context == \"self_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"self_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"other_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"other_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"noAction_self\" and self.action_other == \"noAction_other\":\n",
    "            observed_choice_self = \"noPress_self\"\n",
    "            observed_choice_other = \"noPress_other\"\n",
    "            \n",
    "            if self.context == \"self_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"self_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"other_positive\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"other_negative\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.obs_outcome_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "\n",
    "        obs = [observed_outcome, observed_choice_self, observed_choice_other]\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_inference_loop(my_agent, my_env, T, verbose):\n",
    "    \n",
    "    \"\"\" Initialize the first observation \"\"\"\n",
    "    obs_label = [\"outcome_start\", \"start_self\", \"start_other\"]  \n",
    "    obs = [obs_outcome_names.index(obs_label[0]), obs_choice_self_names.index(obs_label[1]), obs_choice_other_names.index(obs_label[2])]\n",
    "    \n",
    "    first_choice = obs_choice_self_names.index(obs_label[1])\n",
    "    choice_hist = np.zeros((3,T+1))\n",
    "    choice_hist[first_choice,0] = 1.0\n",
    "    \n",
    "    belief_self_hist = np.zeros((num_states[0], T))\n",
    "    belief_other_hist = np.zeros((num_states[1], T))\n",
    "    belief_action_self_hist = np.zeros((num_states[2], T))\n",
    "    belief_action_other_hist = np.zeros((num_states[3], T))\n",
    "    \n",
    "    context_hist = np.zeros(T)\n",
    "    outcome_hist = np.zeros((3,T))\n",
    "    \n",
    "    for t in range(T):\n",
    "        context_hist[t] = env.context_names.index(env.context)\n",
    "        qs = my_agent.infer_states(obs)\n",
    "        \n",
    "        belief_self_hist[:,t] = qs[0]\n",
    "        belief_self_hist[:,t] = qs[1]\n",
    "        belief_action_self_hist[:,t] = qs[2]\n",
    "        belief_action_other_hist[:,t] = qs[3]\n",
    "        \n",
    "        outcome_hist[obs[0],t] = 1.0\n",
    "        \n",
    "        if verbose:\n",
    "             utils.plot_beliefs(qs[0], title = f\"Beliefs about the context at time {t}\")\n",
    "#             utils.plot_beliefs(qs[1], title = f\"Beliefs about the action_self at time {t}\")\n",
    "#             utils.plot_beliefs(qs[2], title = f\"Beliefs about the action_other at time {t}\")\n",
    "            \n",
    "        q_pi, efe = my_agent.infer_policies_factorized()\n",
    "        chosen_action_id = my_agent.sample_action()\n",
    "        \n",
    "        movement_id = int(chosen_action_id[1])\n",
    "        choice_hist[movement_id,t+1]= 1.0\n",
    "        \n",
    "        choice_action = choice_action_self_names[movement_id]\n",
    "        \n",
    "        obs_label = my_env.step(choice_action)\n",
    "        \n",
    "        obs = [obs_outcome_names.index(obs_label[0]), obs_choice_self_names.index(obs_label[1]), obs_choice_other_names.index(obs_label[2])]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Action at time {t}: {choice_action}')\n",
    "            print(f'Outcome at time {t}: {obs_label[0]}')\n",
    "            print(f'Obs_action_self at time {t}: {obs_label[1]}')\n",
    "            print(f'Obs_action_other at time {t}: {obs_label[2]}')\n",
    "\n",
    "    return choice_hist, belief_hist, context_hist,  belief_action_other_hist, outcome_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_some_choices_beliefs(choice_hist, belief_hist, context_hist, pad_val=5.0):\n",
    "    T = choice_hist.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (15,13))\n",
    "    \n",
    "    axes[0].imshow(choice_hist[:,:-1], cmap = 'gray') \n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_yticks(ticks = range(3))\n",
    "    axes[0].set_yticklabels(labels = choice_action_self_names)\n",
    "    axes[0].set_title('Actions produced by Self over time')\n",
    "    \n",
    "    axes[1].imshow(belief_hist, cmap = 'gray')\n",
    "    axes[1].set_xlabel('Timesteps')\n",
    "    axes[1].set_yticks(ticks = range(5))\n",
    "    axes[1].set_yticklabels(labels = context_names)\n",
    "    axes[1].set_title('Beliefs over time')\n",
    "    axes[1].scatter(np.arange(T-1), context_hist, c = 'r', s = 50)\n",
    "    \n",
    "    fig.tight_layout(pad=pad_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_choices_beliefs(choice_hist, choice_o_hist, belief_self_hist, belief_other_hist, context_hist, belief_action_other_hist, outcome_hist, pad_val=1.0):\n",
    "    \n",
    "    print(f'Experimental Condition (or Context): {env.expcondition}')\n",
    "    T = choice_hist.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 5, ncols = 1, figsize = (15,20))\n",
    "    \n",
    "    axes[0].imshow(belief_self_hist, cmap = 'gray')\n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_yticks(ticks = range(num_states[0]))\n",
    "    axes[0].set_yticklabels(labels = context_self_names)\n",
    "    axes[0].set_title('Beliefs_self over time')\n",
    "#     axes[0].scatter(np.arange(T-1), context_hist, c = 'r', s = 50)\n",
    "\n",
    "    axes[1].imshow(belief_other_hist, cmap = 'gray')\n",
    "    axes[1].set_xlabel('Timesteps')\n",
    "    axes[1].set_yticks(ticks = range(num_states[1]))\n",
    "    axes[1].set_yticklabels(labels = context_other_names)\n",
    "    axes[1].set_title('Beliefs_other over time')\n",
    "\n",
    "    axes[2].imshow(choice_hist[:,:-1], cmap = 'gray') \n",
    "    axes[2].set_xlabel('Timesteps')\n",
    "    axes[2].set_yticks(ticks = range(num_states[2]))\n",
    "    axes[2].set_yticklabels(labels = action_self_names)\n",
    "    axes[2].set_title('Actions produced by Self over time')\n",
    "    \n",
    "    axes[3].imshow(belief_action_other_hist[:,:-1], cmap = 'gray') \n",
    "    axes[3].set_xlabel('Timesteps')\n",
    "    axes[3].set_yticks(ticks = range(num_states[3]))\n",
    "    axes[3].set_yticklabels(labels = action_other_names)\n",
    "    axes[3].set_title('Actions produced by Other over time')\n",
    "\n",
    "    axes[4].imshow(outcome_hist[:,:-1], cmap = 'gray') \n",
    "    axes[4].set_xlabel('Timesteps')\n",
    "    axes[4].set_yticks(ticks = range(num_obs[0]))\n",
    "    axes[4].set_yticklabels(labels = obs_outcome_names)\n",
    "    axes[4].set_title('Outcomes observed over time')\n",
    "\n",
    "    fig.tight_layout(pad=pad_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'observed_outcome' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9t/d2mgxx091gn_mrd1r2m5_tw40000gq/T/ipykernel_10962/405126841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmy_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_factor_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA_factor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mchoice_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbelief_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbelief_action_other_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutcome_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_active_inference_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/9t/d2mgxx091gn_mrd1r2m5_tw40000gq/T/ipykernel_10962/2717350752.py\u001b[0m in \u001b[0;36mrun_active_inference_loop\u001b[0;34m(my_agent, my_env, T, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mchoice_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice_action_self_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmovement_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mobs_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobs_outcome_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_choice_self_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_choice_other_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/9t/d2mgxx091gn_mrd1r2m5_tw40000gq/T/ipykernel_10962/1893332318.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mobserved_outcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_outcome_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobserved_outcome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved_choice_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved_choice_other\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'observed_outcome' referenced before assignment"
     ]
    }
   ],
   "source": [
    "p_outcome_env = 1.0\n",
    "p_action_other_env = 0.5\n",
    "\n",
    "env = AgencyTask(p_action_other = p_action_other_env, p_outcome = p_outcome_env)\n",
    "\n",
    "T = 15\n",
    "\n",
    "A,A_factor_list = create_A(p_outcome = 1.0)\n",
    "B = create_B()\n",
    "C = create_C(outcomepref = 5.0, actionpref = 0.0, noactionpref = 0.0)\n",
    "D = create_D()\n",
    "my_agent = Agent(A=A, B=B, C=C, D=D, A_factor_list=A_factor_list)\n",
    "\n",
    "choice_hist, belief_hist, context_hist, belief_action_other_hist, outcome_hist = run_active_inference_loop(my_agent, env, T = T, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_choices_beliefs(choice_hist, belief_hist, context_hist, belief_action_other_hist, outcome_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
