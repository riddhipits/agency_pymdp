{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active inference model of agency task (basic)\n",
    "\n",
    "### prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install inferactively-pymdp\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pymdp\n",
    "\n",
    "from pymdp import utils \n",
    "from pymdp import maths\n",
    "from pymdp.maths import softmax\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specifying the states and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_names = ['human_positive', 'human_negative', \n",
    "                 'comp_positive', 'comp_negative', 'zero']\n",
    "choice_names = ['nothing','humanAction_compAction', 'humanAction_compNoAction', \n",
    "                'humanNoAction_compAction', 'humanNoAction_compNoAction']\n",
    "\n",
    "\"\"\" Define `num_states` and `num_factors` below \"\"\"\n",
    "num_states = [len(context_names), len(choice_names)]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "context_action_names = ['do_nothing']\n",
    "choice_action_names = ['nothing', 'Action_compAction', 'Action_compNoAction',\n",
    "                       'NoAction_compAction', 'NoAction_compNoAction']\n",
    "\n",
    "\"\"\" Define `num_controls` below \"\"\"\n",
    "num_controls = [len(context_action_names), len(choice_action_names)]\n",
    "\n",
    "outcome_obs_names = ['nothing','outcome_present', 'outcome_absent']\n",
    "choice_obs_names = ['nothing', 'humanAction_compAction', 'humanAction_compNoAction',\n",
    "                    'humanNoAction_compAction', 'humanNoAction_compNoAction']\n",
    "\n",
    "\"\"\" Define `num_obs` and `num_modalities` below \"\"\"\n",
    "num_obs = [len(outcome_obs_names), len(choice_obs_names)]\n",
    "num_modalities = len(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_A(p_outcome=1.0):\n",
    "    \n",
    "    A = utils.initialize_empty_A(num_obs, num_states)\n",
    "    \n",
    "    '''likelihood matrix for outcome modality'''\n",
    "    \n",
    "    p_outcome = 1.0 # probability of outcome occurring, according to the agent's generative model\n",
    "    \n",
    "    A_outcome = np.zeros( (len(outcome_obs_names), len(context_names), len(choice_names)) )\n",
    "    \n",
    "    for choice_id, choice_name in enumerate(choice_names):\n",
    "        \n",
    "        if choice_name == \"nothing\":\n",
    "            A_outcome[0,:,choice_id] = 1.0\n",
    "        elif choice_name == \"humanAction_compAction\":\n",
    "            A_outcome[:,:,choice_id] = [[0, 0, 0, 0, 0], \n",
    "                                        [p_outcome, 1.0 - p_outcome, p_outcome, 1.0 - p_outcome, .5], \n",
    "                                        [1.0 - p_outcome, p_outcome, 1.0 - p_outcome, p_outcome, .5]]\n",
    "        elif choice_name == \"humanAction_compNoAction\":\n",
    "            A_outcome[:,:,choice_id] = [[0, 0, 0, 0, 0],\n",
    "                                        [p_outcome, 1.0 - p_outcome, 1.0 - p_outcome, p_outcome, .5],\n",
    "                                        [1.0 - p_outcome, p_outcome, p_outcome, 1.0 - p_outcome, .5]]\n",
    "        elif choice_name == \"humanNoAction_compAction\":\n",
    "            A_outcome[:,:,choice_id] = [[0, 0, 0, 0, 0],\n",
    "                                        [1.0 - p_outcome, p_outcome, p_outcome, 1.0 - p_outcome, .5],\n",
    "                                        [p_outcome, 1.0 - p_outcome, 1.0 - p_outcome, p_outcome, .5]]\n",
    "        elif choice_name == \"humanNoAction_compNoAction\":\n",
    "            A_outcome[:,:,choice_id] = [[0, 0, 0, 0, 0],\n",
    "                                        [1.0 - p_outcome, p_outcome, 1.0 - p_outcome, p_outcome, .5],\n",
    "                                        [p_outcome, 1.0 - p_outcome, p_outcome, 1.0 - p_outcome, .5]]\n",
    "    \n",
    "    '''likelihood matrix for proprioception modality'''        \n",
    "    A_choice = np.zeros((len(choice_obs_names), len(context_names), len(choice_names)))\n",
    "    \n",
    "    for choice_id in range(len(choice_names)):\n",
    "        A_choice[choice_id, :, choice_id] = 1.0\n",
    "        \n",
    "    A[0], A[1] = A_outcome, A_choice\n",
    "    \n",
    "    return A\n",
    "\n",
    "# A = create_A(p_outcome = 1.0)\n",
    "# utils.plot_likelihood(A[0][:,:,0], title = \"Probability of outcome, given nothing\")\n",
    "# utils.plot_likelihood(A[0][:,:,1], title = \"Probability of outcome, given humanAction_compAction\")\n",
    "# utils.plot_likelihood(A[0][:,:,2], title = \"Probability of outcome, given humanAction_compNoAction\")\n",
    "# utils.plot_likelihood(A[0][:,:,3], title = \"Probability of outcome, given humanNoAction_compAction\")\n",
    "# utils.plot_likelihood(A[0][:,:,4], title = \"Probability of outcome, given humanNoAction_compNoAction\")\n",
    "# utils.plot_likelihood(A[1][:,0,:], title=\"Mapping between sensed states and true states for Actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_B(p_change = 0.0):\n",
    "    # `p_change`: probability of the context changing\n",
    "    B = utils.initialize_empty_B(num_states, num_states)\n",
    "    \n",
    "    B_context = np.zeros( (len(context_names), len(context_names), len(context_action_names)) )\n",
    "    B_context[:,:,0] = np.eye(len(context_names))\n",
    "    \n",
    "    B_choice = np.zeros( (len(choice_names), len(choice_names), len(choice_action_names)) )\n",
    "    \n",
    "    for choice_i in range(len(choice_names)):\n",
    "        B_choice[choice_i, :, choice_i] = 1.0\n",
    "        \n",
    "    B[0], B[1] = B_context, B_choice\n",
    "    \n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_C(reward = 1.0, pun = 0.0):\n",
    "    \n",
    "    # define reward and punishment values mapping onto outcome_present and outcome_absent preferences\n",
    "    \n",
    "    C = utils.obj_array_zeros(num_obs)\n",
    "    C[0] = np.array([0.0, reward, pun])\n",
    "    \n",
    "    return C\n",
    "\n",
    "# C = create_C(reward = 1.0, pun = 0.0)\n",
    "# utils.plot_beliefs(softmax(C[0]), title = \"Prior preferences for outcome_present and outcome_absent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_D(p_context=1/float(num_states[0])):\n",
    "    \n",
    "    D = utils.obj_array(num_factors)\n",
    "    \n",
    "    D_context = np.ones(num_states[0])/float(num_states[0])\n",
    "    D[0] = D_context \n",
    "    \n",
    "    D_choice = np.ones(num_states[1])/float(num_states[1])\n",
    "    D[1] = D_choice\n",
    "    \n",
    "    return D\n",
    "\n",
    "# D = create_D(p_context=1/float(num_states[0]))\n",
    "# utils.plot_beliefs(softmax(D[0]), title = \"Prior beliefs about probability of the contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgencyTask(object):\n",
    "    \n",
    "    def __init__(self, context = None, p_outcome = 1.0):\n",
    "        \n",
    "        self.context_names = ['human_positive', 'human_negative',\n",
    "                              'comp_positive', 'comp_negative', 'zero']\n",
    "        if context == None:\n",
    "            self.context = self.context_names[utils.sample(np.ones(num_states[0])/float(num_states[0]))] # randomly sample which context is selected\n",
    "        else:\n",
    "            self.context = context\n",
    "            \n",
    "        self.p_outcome = p_outcome\n",
    "        \n",
    "        self.outcome_obs_names = ['nothing', 'outcome_present', 'outcome_absent']\n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        if action == \"nothing\":\n",
    "            observed_choice = \"nothing\"\n",
    "            \n",
    "            if self.context == \"human_positive\":\n",
    "                observed_outcome = \"nothing\"\n",
    "            elif self.context == \"human_negative\":\n",
    "                observed_outcome = \"nothing\"\n",
    "            elif self.context == \"comp_positive\":\n",
    "                observed_outcome = \"nothing\"\n",
    "            elif self.context == \"comp_negative\":\n",
    "                observed_outcome = \"nothing\"\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = \"nothing\"\n",
    "                \n",
    "        elif action == \"Action_compAction\":\n",
    "            observed_choice = \"humanAction_compAction\"\n",
    "            \n",
    "            if self.context == \"human_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"human_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"comp_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"comp_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"Action_compNoAction\":\n",
    "            observed_choice = \"humanAction_compNoAction\"\n",
    "            \n",
    "            if self.context == \"human_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"human_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"comp_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"comp_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"NoAction_compAction\":\n",
    "            observed_choice = \"humanNoAction_compAction\"\n",
    "            \n",
    "            if self.context == \"human_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"human_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"comp_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"comp_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "                \n",
    "        elif action == \"humanNoAction_compNoAction\":\n",
    "            observed_choice = \"humanNoAction_compNoAction\"\n",
    "            \n",
    "            if self.context == \"human_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"human_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"comp_positive\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 1.0-self.p_outcome, self.p_outcome]))]\n",
    "            elif self.context == \"comp_negative\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, self.p_outcome, 1.0-self.p_outcome]))]\n",
    "            elif self.context == \"zero\":\n",
    "                observed_outcome = self.outcome_obs_names[utils.sample(np.array([0.0, 0.5, 0.5]))]\n",
    "\n",
    "        obs = [observed_outcome, observed_choice]\n",
    "\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_active_inference_loop(my_agent, my_env, T = 5, verbose = False):\n",
    "    \n",
    "    \"\"\" Initialize the first observation \"\"\"\n",
    "    obs_label = [\"nothing\", \"nothing\"]  \n",
    "    obs = [outcome_obs_names.index(obs_label[0]), choice_obs_names.index(obs_label[1])]\n",
    "    \n",
    "    first_choice = choice_obs_names.index(obs_label[1])\n",
    "    choice_hist = np.zeros((5,T+1))\n",
    "    choice_hist[first_choice,0] = 1.0\n",
    "    \n",
    "    belief_hist = np.zeros((5, T))\n",
    "    context_hist = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        context_hist[t] = env.context_names.index(env.context)\n",
    "        qs = my_agent.infer_states(obs)\n",
    "        \n",
    "        belief_hist[:,t] = qs[0]\n",
    "        \n",
    "        if verbose:\n",
    "            utils.plot_beliefs(qs[0], title = f\"Beliefs about the context at time {t}\")\n",
    "            \n",
    "        q_pi, efe = my_agent.infer_policies()\n",
    "        chosen_action_id = my_agent.sample_action()\n",
    "        \n",
    "        movement_id = int(chosen_action_id[1])\n",
    "        choice_hist[movement_id,t+1]= 1.0\n",
    "        \n",
    "        choice_action = choice_action_names[movement_id]\n",
    "        \n",
    "        obs_label = my_env.step(choice_action)\n",
    "        \n",
    "        obs = [outcome_obs_names.index(obs_label[0]), choice_obs_names.index(obs_label[1])]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Action at time {t}: {choice_action}')\n",
    "            print(f'Outcome at time {t}: {obs_label[0]}')\n",
    "\n",
    "    return choice_hist, belief_hist, context_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_choices_beliefs(choice_hist, belief_hist, context_hist, pad_val=5.0):\n",
    "    T = choice_hist.shape[1]\n",
    "    fig, axes = plt.subplots(nrows = 2, ncols = 1, figsize = (15,13))\n",
    "    \n",
    "    axes[0].imshow(choice_hist[:,:-1], cmap = 'gray') \n",
    "    axes[0].set_xlabel('Timesteps')\n",
    "    axes[0].set_yticks(ticks = range(5))\n",
    "    axes[0].set_yticklabels(labels = choice_action_names)\n",
    "    axes[0].set_title('Choices over time')\n",
    "    \n",
    "    axes[1].imshow(belief_hist, cmap = 'gray')\n",
    "    axes[1].set_xlabel('Timesteps')\n",
    "    axes[1].set_yticks(ticks = range(5))\n",
    "    axes[1].set_yticklabels(labels = ['human_positive', 'human_negative', 'comp_positive', 'comp_negative', 'zero'])\n",
    "    axes[1].set_title('Beliefs over time')\n",
    "    axes[1].scatter(np.arange(T-1), context_hist, c = 'r', s = 50)\n",
    "    \n",
    "    fig.tight_layout(pad=pad_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_outcome_env = 1.0\n",
    "\n",
    "env = AgencyTask(p_outcome = p_outcome_env)\n",
    "\n",
    "T = 15\n",
    "\n",
    "A = create_A(p_outcome = 1.0)\n",
    "B = create_B(p_change = 0.0)\n",
    "C = create_C(reward = 1.0, pun = 0.0)\n",
    "D = create_D(p_context=1/float(num_states[0]))\n",
    "my_agent = Agent(A=A, B=B, C=C, D=D)\n",
    "\n",
    "choice_hist, belief_hist, context_hist = run_active_inference_loop(my_agent, env, T = T, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_choices_beliefs(choice_hist, belief_hist, context_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
